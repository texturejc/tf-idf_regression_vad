{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72993c4d-b4c6-445e-9ac0-be4c439597ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jamescarney/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jamescarney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "import re\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from IPython.display import IFrame\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from scipy.spatial import distance\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "import string\n",
    "punct = list(string.punctuation)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import scipy as sp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import r_regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99dff58a-f53c-4ea3-9d02-dce5744cf8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"books_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "119296f1-864c-4db8-b976-2d5b0ef69353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'redditor', 'score', 'subreddit', 'text', 'title', 'type',\n",
       "       'proc_text', 'proc_title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d9c110-278a-4f38-854d-2ef85c795608",
   "metadata": {},
   "source": [
    "# Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773bc2f4-aac8-4d3f-896d-ad5708929aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_tokens(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(i).lower() for i in words]\n",
    "    lemmas = [i for i in lemmas if i not in stops and i not in punct]\n",
    "    return lemmas\n",
    "\n",
    "text = [i for i in data['text']]\n",
    "vectorizer = TfidfVectorizer(tokenizer=good_tokens)\n",
    "vectors = vectorizer.fit_transform(text)\n",
    "vectors = vectors.todense().tolist()\n",
    "df = pd.DataFrame(\n",
    "    vectors,columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "terms = df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71462e5-59b3-4329-9519-b6dfbde93bab",
   "metadata": {},
   "source": [
    "# VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb7f23-0265-4539-a28f-20662c4d2319",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad = pd.read_csv('vad.csv', index_col = 0)  #VAD norms\n",
    "vad = vad[[\"V.Mean.Sum\", \"A.Mean.Sum\", \"D.Mean.Sum\"]]\n",
    "vad.columns = ['valence', 'arousal', 'dominance']\n",
    "\n",
    "\n",
    "def vad_data(text):\n",
    "    word_list = good_tokens(text)\n",
    "    words = []\n",
    "    norms = []\n",
    "\n",
    "    try:\n",
    "    \n",
    "        for i in word_list:\n",
    "            if i in vad.index:\n",
    "                norms.append(vad.loc[i])\n",
    "                words.append(i)\n",
    "            else:\n",
    "                pass\n",
    "    except:\n",
    "        norms.append(pd.Series([np.nan, np.nan, np.nan)\n",
    "                words.append(i)\n",
    "    norms_vad = pd.DataFrame(norms, index = words)\n",
    "    return norms_vad.mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68d2699-51af-48aa-9733-d9230be9cab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression() #Call the model\n",
    "\n",
    "X = data[[\"IVs\"]]\n",
    "y = data['DV']\n",
    "\n",
    "reg = linreg.fit(X,y) #Fit the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP] *",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
